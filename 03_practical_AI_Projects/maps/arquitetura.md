# ğŸ§  Arquitetura NPL (Natural Language Processing)

## ğŸ“š Tipos de Arquiteturas

### ğŸ”„ RNN (Recurrent Neural Networks)
- **PropÃ³sito**: Projetada para lidar com dados sequenciais
- **CaracterÃ­stica**: Processa informaÃ§Ãµes em sequÃªncia temporal

### ğŸ§  LSTM (Long Short-Term Memory)
- **FunÃ§Ã£o**: Lembrar informaÃ§Ãµes por mais tempo
- **Componentes**:
  - `cell state` (memÃ³ria de longo prazo)
  - `hidden state`

### âš¡ GRU (Gated Recurrent Unit)
- **FunÃ§Ã£o**: Mesma funÃ§Ã£o do LSTM, usando menos recursos
- **Componentes**:
  - `hidden state`

---

## ğŸ¤– Transformers

> Criada para lidar com dados sequenciais, como texto, mas sem depender da sequÃªncia. Usam o **Self-Attention** para conectar tudo com tudo.

### ğŸ“¥ Encoder
**FunÃ§Ã£o**: Entender o input, seja uma frase, uma imagem ou o que for. Ele nÃ£o gera resposta. Ele observa, absorve, interpreta.

#### ğŸ”§ O que ele faz:
1. Recebe a sequÃªncia de entrada (ex: "O cÃ©u estÃ¡ azul")
2. Aplica `embeddings` + `positional encoding` â†’ transforma palavras em vetores com posiÃ§Ã£o
3. Passa isso por vÃ¡rios blocos de atenÃ§Ã£o (`Self-Attention`) e `Feed-Forward Networks`
4. **Resultado**: Um conjunto de vetores ricos em contexto â†’ chamados de "context vectors"

#### ğŸ’¡ Exemplo: BERT
**BERT** (Bidirectional Encoder Representations for Transformers)

- **Serve para**: Compreender, classificar, responder perguntas, extrair entidades, etc.
- **NÃ£o faz**: NÃ£o prevÃª prÃ³xima palavra
- **Treinamento**: Usa MÃ¡scara (`Masked Language Modeling`) â†’ ele esconde palavras e tenta adivinhar com base no contexto

---

### ğŸ“¤ Decoder
**FunÃ§Ã£o**: Pega essa informaÃ§Ã£o codificada e gera a resposta, palavra por palavra, passo a passo. Ele Ã© o output.

#### ğŸ”§ O que ele faz:
1. ComeÃ§a com um token especial `[START]` (ou o que jÃ¡ foi gerado)
2. Usa `self-attention` para olhar para o que ele jÃ¡ produziu
3. Usa `encoder-decoder attention` para olhar para o que o encoder entendeu
4. Passa por um `feed-forward` e gera o prÃ³ximo token
5. Repete atÃ© chegar no token `[END]`

#### ğŸ’¡ Exemplo: GPT
- **Arquitetura**: Usa somente o decoder, com uma tÃ©cnica chamada `masked self-attention` (ele sÃ³ olha pro passado)
- **Serve para**: Gerar texto, completar sentenÃ§as, criar respostas, escrever cÃ³digo, etc.
- **LimitaÃ§Ãµes**:
  - Ele nÃ£o entende texto inteiro ao mesmo tempo, sÃ³ prevÃª palavra por palavra
  - Precisa de muito contexto anterior para manter coerÃªncia
  - Mais propenso a "alucinar" (inventar coisa), porque nÃ£o entende tudo ao mesmo tempo como o BERT

---

### ğŸ”„ Encoder-Decoder
**Conceito**: O encoder lÃª toda a entrada e gera um "mapa de entendimento". O decoder consulta esse mapa enquanto vai construindo a saÃ­da.

#### ğŸ’¡ Exemplo: T5 (Text-To-Text Transfer Transformer)

##### âœ¨ CaracterÃ­sticas Principais:

**ğŸ¯ Formato Ãºnico para todas as tarefas**
- Um Ãºnico modelo pode fazer MUITA coisa
- Isso facilita fine-tuning

**ğŸ­ PrÃ©-treinamento com "span corruption"**
- Diferente do BERT que mascara sÃ³ UMA palavra, o T5 mascara TRECHOS inteiros (chamado span corruption)
- Isso ensina o modelo a reconstruir ideias complexas, nÃ£o sÃ³ palavras

**ğŸš€ Flexibilidade absurda**
- Pode usar o mesmo modelo para traduÃ§Ã£o, QA, inferÃªncia, resumo, classificaÃ§Ã£o, etc.

**ğŸ“ InstruÃ§Ã£o explÃ­cita embutida no input**
- Ele precisa que vocÃª diga o que quer dele
- Isso Ã© a base de tudo que virou prompt engineering depois

**ğŸŒŸ Base para**:
- `FLAN-T5` â†’ versÃ£o refinada, com fine-tuning em mÃºltiplas tarefas usando instruÃ§Ãµes
- `Alpaca`, `Vicuna`, `LLaMA-Instruction`

**ğŸ“ Treinamento**: Text-to-text com instruÃ§Ãµes

---

#### ğŸ’¡ Exemplo: BART (Bidirectional and Auto-Regressive Transformer)

##### ğŸ”§ Como funciona?

**ğŸ¯ PrÃ©-treinamento com reconstruÃ§Ã£o de texto corrompido**
- Ele aprende a consertar frases bagunÃ§adas

**ğŸ› ï¸ TÃ©cnicas como**:
- Adicionar ruÃ­do (corrupÃ§Ã£o)
- Embaralhar sentenÃ§as
- Apagar palavras aleatÃ³rias

**ğŸ“¥ Encoder (BERT-like)**
- Processa a entrada com compreensÃ£o bidirecional
- Ele "lÃª" o texto todo antes de tomar decisÃµes

**ğŸ“¤ Decoder (GPT-like)**
- Gera a saÃ­da palavra por palavra, prevendo o prÃ³ximo token com base nos anteriores

##### â­ O que o torna especial?

**ğŸ¯ Especialista em tarefas de reconstruÃ§Ã£o de texto**
- Resumo, correÃ§Ã£o gramatical, geraÃ§Ã£o condicional, reescrita

**ğŸ§  Compreende o contexto de forma rica (como BERT)**
- NÃ£o depende sÃ³ do que veio antes (como GPT), mas tambÃ©m do que veio depois
- Isso Ã© ouro para entender o meio da frase

**âœï¸ Gera texto com fluidez (como GPT)**
- O decoder autoregressivo permite respostas coerentes, longas, naturais

---

#### ğŸ’¡ Exemplo: MarianMT (Marian Machine Translation)

##### ğŸ”§ Como ele funciona?

**ğŸ—ï¸ Arquitetura baseada no Transformer (Encoderâ€“Decoder)**
- PrÃ©-treinado em grandes corpora paralelos, como:
  - **OPUS**: banco de dados de traduÃ§Ãµes humanas
- Cada modelo Ã© direcionado para um par de lÃ­nguas (ex: `pt-en`, `en-de`, etc.)

**ğŸ§  Processo**:
- Ele usa o encoder para entender o idioma de origem
- E o decoder para construir a traduÃ§Ã£o, uma palavra por vez, no idioma-alvo

##### â­ O que torna especial?

**ğŸ¯ Treinamento altamente especializado**
- Focado sÃ³ em traduÃ§Ã£o, sem ruÃ­dos de outras tarefas

**âš¡ Modelos pequenos e eficientes**
- Leves o bastante para rodar em CPUs ou atÃ© em mobile, mas ainda potentes

**ğŸŒ MultilÃ­ngue com escalabilidade**
- Pode lidar com centenas de pares linguÃ­sticos

**ğŸ¤— CompatÃ­vel com HuggingFace**
- Prontinho para vocÃª testar com apenas 3 linhas de cÃ³digo

---

## ğŸ”§ Componentes

### ğŸ“ Input

#### ğŸ”¤ TokenizaÃ§Ã£o
> Processo de quebrar um input em partes menores chamadas tokens

**Tipos de TokenizaÃ§Ã£o**:

- **ğŸ“ Word-level**: Cada palavra Ã© um token
- **ğŸ”¤ Character-level**: Cada letra Ã© um token  
- **ğŸ§© Subword**: Cada montante de letras Ã© um token
  - **BPE** (Byte Pair Encoding): NÃ£o respeita espaÃ§os, aprende onde o espaÃ§o deveria estar a partir do padrÃ£o das subpalavras
  - **SentencePiece**: Respeita espaÃ§os

---

### ğŸ¯ Embeddings
> Transformam palavras (tokens) em vetores

#### ğŸ“Š Modelos

**ğŸ”’ EstÃ¡ticos** (cada palavra tem o mesmo vetor, independente do contexto):
- `Word2Vec`
- `GloVe`

**ğŸ”„ Contextual**:
- `ELMo`

---

### ğŸ’¾ Estados Internos
- `hidden state`
- `cell state` (memÃ³ria a longo prazo)

---

## âš™ï¸ Componentes dos Transformers

### ğŸ“ Positional Encoding
> Saber a posiÃ§Ã£o das palavras numa frase

### ğŸ‘ï¸ Self-Attention

#### ğŸ”„ TransformaÃ§Ã£o
Cada palavra vira:
- **ğŸ” Query** (o que quer saber)
- **ğŸ”‘ Key** (o que oferece)  
- **ğŸ’ Value** (o que carrega)

#### ğŸ“Š Scoring (PontuaÃ§Ã£o)
A palavra pega sua query e compara com a key de outras, isso gera um peso. Quanto maior o peso, maior a relevÃ¢ncia da palavra.

#### âš–ï¸ CombinaÃ§Ã£o ponderada (AtenÃ§Ã£o)
Esse peso Ã© multiplicado ao value, dando mais importÃ¢ncia para as mais relevantes. Resultado: um vetor contextualizado.

### ğŸ­ Multi-Head Attention
> Analisa vÃ¡rios sentidos da frase ao mesmo tempo com mÃºltiplas "cabeÃ§as". 
> 
> **PS**: Todos esses sentidos sÃ£o "cabeÃ§as" que passam por um filtro para definir o real.

### ğŸ”§ Feed-Forward Network
> Refina o resultado da atenÃ§Ã£o, prepara para os prÃ³ximos blocos.

### ğŸ”„ Residual Coding
> MantÃ©m o que foi aprendido antes, criando atalhos diretos para evitar "esquecimento".

---

## ğŸ“Š VisualizaÃ§Ã£o

### ğŸ“ˆ PCA (Principal Component Analysis)
- **âœ… Utilidade**: Reduz as dimensÃµes dos vetores mantendo o mÃ¡ximo de variÃ¢ncia. Ideal para compreensÃ£o rÃ¡pida e aceleraÃ§Ã£o de visualizaÃ§Ã£o
- **âŒ LimitaÃ§Ãµes**: NÃ£o lida bem com padrÃµes nÃ£o lineares, nÃ£o preserva estrutura local

### ğŸ¯ t-SNE (t-distributed Stochastic Neighbor Embedding)
- **âœ… Utilidade**: Focado em manter amizades locais (clusters prÃ³ximos). Ã“timo para analisar relaÃ§Ãµes semÃ¢nticas entre palavras
- **âŒ LimitaÃ§Ãµes**: NÃ£o preserva estrutura global, resultados instÃ¡veis, lento com grandes volumes. Serve para visualizaÃ§Ã£o, nÃ£o para input de modelo

### ğŸ—ºï¸ UMAP (Uniform Manifold Approximation and Projection)
- **âœ… Utilidade**: Preserva estrutura local e global. RÃ¡pido. Usado em pipelines reais de deep learning, NLP, bioinformÃ¡tica
- **ğŸš€ FunÃ§Ãµes extras**: Pode ser usado para visualizaÃ§Ã£o, compressÃ£o, clustering e embedding em produÃ§Ã£o

---

## ğŸ“š Resumo

Este documento apresenta uma visÃ£o abrangente das principais arquiteturas de NPL, desde RNNs tradicionais atÃ© os modernos Transformers, incluindo seus componentes fundamentais e tÃ©cnicas de visualizaÃ§Ã£o. Cada arquitetura tem suas caracterÃ­sticas especÃ­ficas e casos de uso ideais, formando o ecossistema atual do processamento de linguagem natural.

